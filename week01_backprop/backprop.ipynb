{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "backprop.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/legchikov/Practical_DL/blob/fall18/week01_backprop/backprop.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "NBs_E4Vvk7hN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Your very own neural network\n",
        "\n",
        "In this notebook, we're going to build a neural network using naught but pure numpy and steel nerves. It's going to be fun, I promise!\n",
        "\n",
        "![img](https://github.com/legchikov/Practical_DL/blob/fall18/week01_backprop/frank.png?raw=1)"
      ]
    },
    {
      "metadata": {
        "id": "Ew5CmUIfk7iQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Here goes our main class: a layer that can .forward() and .backward()."
      ]
    },
    {
      "metadata": {
        "id": "WHwEV24ok7hc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "import numpy as np\n",
        "np.random.seed(42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RsxxMr9ZquyX",
        "colab_type": "code",
        "outputId": "6bbd5691-b154-46f2-f510-7f8acf1f7830",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 220
        }
      },
      "cell_type": "code",
      "source": [
        "# Download file to google colab\n",
        "from google.colab import files\n",
        "src = list(files.upload().values())[0]\n",
        "open('util.py','wb').write(src)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-a2b7dea4-4c19-4727-aa85-c0c523a75361\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-a2b7dea4-4c19-4727-aa85-c0c523a75361\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-d7063fbf15a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'util.py'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "IUUlJsOpq-oD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Remove file or folder\n",
        "!rm <filename>\n",
        "!rm -rf <folder>"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "F6YsSboTk7is",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Layer:\n",
        "    \"\"\"\n",
        "    A building block. Each layer is capable of performing two things:\n",
        "    \n",
        "    - Process input to get output:           output = layer.forward(input)\n",
        "    \n",
        "    - Propagate gradients through itself:    grad_input = layer.backward(input, grad_output)\n",
        "    \n",
        "    Some layers also have learnable parameters which they update during layer.backward.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        \"\"\"Here you can initialize layer parameters (if any) and auxiliary stuff.\"\"\"\n",
        "        # A dummy layer does nothing\n",
        "        pass\n",
        "    \n",
        "    def forward(self, input):\n",
        "        \"\"\"\n",
        "        Takes input data of shape [batch, input_units], returns output data [batch, output_units]\n",
        "        \"\"\"\n",
        "        \n",
        "        return input\n",
        "\n",
        "    def backward(self, input, grad_output):\n",
        "        \"\"\"\n",
        "        Performs a backpropagation step through the layer, with respect to the given input.\n",
        "        \n",
        "        To compute loss gradients w.r.t input, you need to apply chain rule (backprop):\n",
        "        \n",
        "        d loss / d x  = (d loss / d layer) * (d layer / d x)\n",
        "        \n",
        "        Luckily, you already receive d loss / d layer as input, so you only need to multiply it by d layer / d x.\n",
        "        \n",
        "        If your layer has parameters (e.g. dense layer), you also need to update them here using d loss / d layer\n",
        "        \"\"\"\n",
        "        # The gradient of a dummy layer is precisely grad_output, but we'll write it more explicitly\n",
        "        num_units = input.shape[1]\n",
        "        \n",
        "        d_layer_d_input = np.eye(num_units)\n",
        "        \n",
        "        return np.dot(grad_output, d_layer_d_input) # chain rule"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kNrkOYqPk7jO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### The road ahead\n",
        "\n",
        "We're going to build a neural network that classifies MNIST digits. To do so, we'll need a few building blocks:\n",
        "- Dense layer - a fully-connected layer, $f(X)=X \\cdot W + \\vec{b}$\n",
        "- ReLU layer (or any other nonlinearity you want)\n",
        "- Loss function - crossentropy\n",
        "- Backprop algorithm - a stochastic gradient descent with backpropageted gradients\n",
        "\n",
        "Let's approach them one at a time.\n"
      ]
    },
    {
      "metadata": {
        "id": "vfxRGiRok7jU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Nonlinearity layer\n",
        "\n",
        "This is the simplest layer you can get: it simply applies a nonlinearity to each element of your network."
      ]
    },
    {
      "metadata": {
        "id": "oKPDFZqLk7kL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class ReLU(Layer):\n",
        "    def __init__(self):\n",
        "        \"\"\"ReLU layer simply applies elementwise rectified linear unit to all inputs\"\"\"\n",
        "        pass\n",
        "    \n",
        "    def forward(self, input):\n",
        "        \"\"\"Apply elementwise ReLU to [batch, input_units] matrix\"\"\"\n",
        "        # <your code. Try np.maximum>\n",
        "        output = np.maximum(0, input)\n",
        "        return output\n",
        "    \n",
        "    def backward(self, input, grad_output):\n",
        "        \"\"\"Compute gradient of loss w.r.t. ReLU input\"\"\"\n",
        "        relu_grad = input > 0\n",
        "        return grad_output*relu_grad        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "U137pq42k7lV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# some tests\n",
        "from util import eval_numerical_gradient\n",
        "x = np.linspace(-1,1,10*32).reshape([10,32])\n",
        "l = ReLU()\n",
        "grads = l.backward(x,np.ones([10,32])/(32*10))\n",
        "numeric_grads = eval_numerical_gradient(lambda x: l.forward(x).mean(), x=x)\n",
        "assert np.allclose(grads, numeric_grads, rtol=1e-3, atol=0),\\\n",
        "    \"gradient returned by your layer does not match the numerically computed gradient\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gcblgUmrk7ld",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Instant primer: lambda functions\n",
        "\n",
        "In python, you can define functions in one line using the `lambda` syntax: `lambda param1, param2: expression`\n",
        "\n",
        "For example: `f = lambda x, y: x+y` is equivalent to a normal function:\n",
        "\n",
        "```\n",
        "def f(x,y):\n",
        "    return x+y\n",
        "```\n",
        "For more information, click [here](http://www.secnetix.de/olli/Python/lambda_functions.hawk).    "
      ]
    },
    {
      "metadata": {
        "id": "jSrmdoZFk7li",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Dense layer\n",
        "\n",
        "Now let's build something more complicated. Unlike nonlinearity, a dense layer actually has something to learn.\n",
        "\n",
        "A dense layer applies affine transformation. In a vectorized form, it can be described as:\n",
        "$$f(X)= X \\cdot W + \\vec b $$\n",
        "\n",
        "Where \n",
        "* X is an object-feature matrix of shape [batch_size, num_features],\n",
        "* W is a weight matrix [num_features, num_outputs] \n",
        "* and b is a vector of num_outputs biases.\n",
        "\n",
        "Both W and b are initialized during layer creation and updated each time backward is called."
      ]
    },
    {
      "metadata": {
        "id": "i4y9mYuak7lk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Dense(Layer):\n",
        "    def __init__(self, input_units, output_units, learning_rate=0.1):\n",
        "        \"\"\"\n",
        "        A dense layer is a layer which performs a learned affine transformation:\n",
        "        f(x) = <x*W> + b\n",
        "        \"\"\"\n",
        "        self.learning_rate = learning_rate\n",
        "        \n",
        "        # initialize weights with small random numbers. We use normal initialization, \n",
        "        # but surely there is something better. Try this once you got it working: http://bit.ly/2vTlmaJ\n",
        "        self.weights = np.random.randn(input_units, output_units)*0.01\n",
        "        self.biases = np.zeros(output_units)\n",
        "        \n",
        "    def forward(self,input):\n",
        "        \"\"\"\n",
        "        Perform an affine transformation:\n",
        "        f(x) = <x*W> + b\n",
        "        \n",
        "        input shape: [batch, input_units]\n",
        "        output shape: [batch, output units]\n",
        "        \"\"\"\n",
        "        return np.dot(input, self.weights) + self.biases\n",
        "    \n",
        "    def backward(self,input,grad_output):\n",
        "        \n",
        "        # compute d f / d x = d f / d dense * d dense / d x\n",
        "        # where d dense/ d x = weights transposed\n",
        "        grad_input = np.dot(grad_output, self.weights.T)\n",
        "        \n",
        "        # compute gradient w.r.t. weights and biases\n",
        "        grad_weights = np.dot(input.T, grad_output)\n",
        "        grad_biases = grad_output.mean(axis=0)*input.shape[0]\n",
        "        \n",
        "        assert grad_weights.shape == self.weights.shape and grad_biases.shape == self.biases.shape\n",
        "        # Here we perform a stochastic gradient descent step. \n",
        "        # Later on, you can try replacing that with something better.\n",
        "        self.weights = self.weights - self.learning_rate * grad_weights\n",
        "        self.biases = self.biases - self.learning_rate * grad_biases\n",
        "        \n",
        "        return grad_input"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VSmnM3Afk7lx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Testing the dense layer\n",
        "\n",
        "Here we have a few tests to make sure your dense layer works properly. You can just run them, get 3 \"well done\"s and forget they ever existed.\n",
        "\n",
        "... or not get 3 \"well done\"s and go fix stuff. If that is the case, here are some tips for you:\n",
        "* Make sure you compute gradients for W and b as __sum of gradients over batch__, not mean over gradients. Grad_output is already divided by batch size.\n",
        "* If you're debugging, try saving gradients in class fields, like \"self.grad_w = grad_w\" or print first 3-5 weights. This helps debugging.\n",
        "* If nothing else helps, try ignoring tests and proceed to network training. If it trains alright, you may be off by something that does not affect network training."
      ]
    },
    {
      "metadata": {
        "id": "lvqodjHQk7l8",
        "colab_type": "code",
        "outputId": "18bd7f4b-cfaa-459e-9968-6de198a8aae4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "l = Dense(128, 150)\n",
        "\n",
        "assert -0.05 < l.weights.mean() < 0.05 and 1e-3 < l.weights.std() < 1e-1,\\\n",
        "    \"The initial weights must have zero mean and small variance. \"\\\n",
        "    \"If you know what you're doing, remove this assertion.\"\n",
        "assert -0.05 < l.biases.mean() < 0.05, \"Biases must be zero mean. Ignore if you have a reason to do otherwise.\"\n",
        "\n",
        "# To test the outputs, we explicitly set weights with fixed values. DO NOT DO THAT IN ACTUAL NETWORK!\n",
        "l = Dense(3,4)\n",
        "\n",
        "x = np.linspace(-1,1,2*3).reshape([2,3])\n",
        "l.weights = np.linspace(-1,1,3*4).reshape([3,4])\n",
        "l.biases = np.linspace(-1,1,4)\n",
        "\n",
        "assert np.allclose(l.forward(x),np.array([[ 0.07272727,  0.41212121,  0.75151515,  1.09090909],\n",
        "                                          [-0.90909091,  0.08484848,  1.07878788,  2.07272727]]))\n",
        "print(\"Well done!\")"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Well done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "dnjqx99fk7mP",
        "colab_type": "code",
        "outputId": "6fa462e3-b8d5-458b-8910-f6de8d986f9b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# To test the grads, we use gradients obtained via finite differences\n",
        "\n",
        "from util import eval_numerical_gradient\n",
        "\n",
        "x = np.linspace(-1,1,10*32).reshape([10,32])\n",
        "l = Dense(32,64,learning_rate=0)\n",
        "\n",
        "numeric_grads = eval_numerical_gradient(lambda x: l.forward(x).sum(),x)\n",
        "grads = l.backward(x,np.ones([10,64]))\n",
        "\n",
        "assert np.allclose(grads,numeric_grads,rtol=1e-3,atol=0), \"input gradient does not match numeric grad\"\n",
        "print(\"Well done!\")"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Well done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "la6ZMnBDk7m1",
        "colab_type": "code",
        "outputId": "5c70506e-6f3e-449d-f1f6-7e4f6951c3af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "#test gradients w.r.t. params\n",
        "def compute_out_given_wb(w,b):\n",
        "    l = Dense(32,64,learning_rate=1)\n",
        "    l.weights = np.array(w)\n",
        "    l.biases = np.array(b)\n",
        "    x = np.linspace(-1,1,10*32).reshape([10,32])\n",
        "    return l.forward(x)\n",
        "    \n",
        "def compute_grad_by_params(w,b):\n",
        "    l = Dense(32,64,learning_rate=1)\n",
        "    l.weights = np.array(w)\n",
        "    l.biases = np.array(b)\n",
        "    x = np.linspace(-1,1,10*32).reshape([10,32])\n",
        "    l.backward(x,np.ones([10,64]) / 10.)\n",
        "    return w - l.weights, b - l.biases\n",
        "    \n",
        "w,b = np.random.randn(32,64), np.linspace(-1,1,64)\n",
        "\n",
        "numeric_dw = eval_numerical_gradient(lambda w: compute_out_given_wb(w,b).mean(0).sum(),w )\n",
        "numeric_db = eval_numerical_gradient(lambda b: compute_out_given_wb(w,b).mean(0).sum(),b )\n",
        "grad_w,grad_b = compute_grad_by_params(w,b)\n",
        "\n",
        "assert np.allclose(numeric_dw,grad_w,rtol=1e-3,atol=0), \"weight gradient does not match numeric weight gradient\"\n",
        "assert np.allclose(numeric_db,grad_b,rtol=1e-3,atol=0), \"weight gradient does not match numeric weight gradient\"\n",
        "print(\"Well done!\")"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Well done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "V9ULmwlOk7nI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### The loss function\n",
        "\n",
        "Since we want to predict probabilities, it would be logical for us to define softmax nonlinearity on top of our network and compute loss given predicted probabilities. However, there is a better way to do so.\n",
        "\n",
        "If you write down the expression for crossentropy as a function of softmax logits (a), you'll see:\n",
        "\n",
        "$$ loss = - log \\space {e^{a_{correct}} \\over {\\underset i \\sum e^{a_i} } } $$\n",
        "\n",
        "If you take a closer look, ya'll see that it can be rewritten as:\n",
        "\n",
        "$$ loss = - a_{correct} + log {\\underset i \\sum e^{a_i} } $$\n",
        "\n",
        "It's called Log-softmax and it's better than naive log(softmax(a)) in all aspects:\n",
        "* Better numerical stability\n",
        "* Easier to get derivative right\n",
        "* Marginally faster to compute\n",
        "\n",
        "So why not just use log-softmax throughout our computation and never actually bother to estimate probabilities.\n",
        "\n",
        "Here you are! We've defined the both loss functions for you so that you could focus on neural network part."
      ]
    },
    {
      "metadata": {
        "id": "c4XSxS8Uk7nM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def softmax_crossentropy_with_logits(logits,reference_answers):\n",
        "    \"\"\"Compute crossentropy from logits[batch,n_classes] and ids of correct answers\"\"\"\n",
        "    logits_for_answers = logits[np.arange(len(logits)),reference_answers]\n",
        "    \n",
        "    xentropy = - logits_for_answers + np.log(np.sum(np.exp(logits),axis=-1))\n",
        "    \n",
        "    return xentropy\n",
        "\n",
        "def grad_softmax_crossentropy_with_logits(logits,reference_answers):\n",
        "    \"\"\"Compute crossentropy gradient from logits[batch,n_classes] and ids of correct answers\"\"\"\n",
        "    ones_for_answers = np.zeros_like(logits)\n",
        "    ones_for_answers[np.arange(len(logits)),reference_answers] = 1\n",
        "    \n",
        "    softmax = np.exp(logits) / np.exp(logits).sum(axis=-1,keepdims=True)\n",
        "    \n",
        "    return (- ones_for_answers + softmax) / logits.shape[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TqrCgSFafIvW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1717
        },
        "outputId": "4c6b5b0d-8f00-4aef-b46a-6ecfc7cbd202"
      },
      "cell_type": "code",
      "source": [
        "logits = np.linspace(-1,1,500).reshape([50,10])\n",
        "logits"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-1.        , -0.99599198, -0.99198397, -0.98797595, -0.98396794,\n",
              "        -0.97995992, -0.9759519 , -0.97194389, -0.96793587, -0.96392786],\n",
              "       [-0.95991984, -0.95591182, -0.95190381, -0.94789579, -0.94388778,\n",
              "        -0.93987976, -0.93587174, -0.93186373, -0.92785571, -0.9238477 ],\n",
              "       [-0.91983968, -0.91583166, -0.91182365, -0.90781563, -0.90380762,\n",
              "        -0.8997996 , -0.89579158, -0.89178357, -0.88777555, -0.88376754],\n",
              "       [-0.87975952, -0.8757515 , -0.87174349, -0.86773547, -0.86372745,\n",
              "        -0.85971944, -0.85571142, -0.85170341, -0.84769539, -0.84368737],\n",
              "       [-0.83967936, -0.83567134, -0.83166333, -0.82765531, -0.82364729,\n",
              "        -0.81963928, -0.81563126, -0.81162325, -0.80761523, -0.80360721],\n",
              "       [-0.7995992 , -0.79559118, -0.79158317, -0.78757515, -0.78356713,\n",
              "        -0.77955912, -0.7755511 , -0.77154309, -0.76753507, -0.76352705],\n",
              "       [-0.75951904, -0.75551102, -0.75150301, -0.74749499, -0.74348697,\n",
              "        -0.73947896, -0.73547094, -0.73146293, -0.72745491, -0.72344689],\n",
              "       [-0.71943888, -0.71543086, -0.71142285, -0.70741483, -0.70340681,\n",
              "        -0.6993988 , -0.69539078, -0.69138277, -0.68737475, -0.68336673],\n",
              "       [-0.67935872, -0.6753507 , -0.67134269, -0.66733467, -0.66332665,\n",
              "        -0.65931864, -0.65531062, -0.65130261, -0.64729459, -0.64328657],\n",
              "       [-0.63927856, -0.63527054, -0.63126253, -0.62725451, -0.62324649,\n",
              "        -0.61923848, -0.61523046, -0.61122244, -0.60721443, -0.60320641],\n",
              "       [-0.5991984 , -0.59519038, -0.59118236, -0.58717435, -0.58316633,\n",
              "        -0.57915832, -0.5751503 , -0.57114228, -0.56713427, -0.56312625],\n",
              "       [-0.55911824, -0.55511022, -0.5511022 , -0.54709419, -0.54308617,\n",
              "        -0.53907816, -0.53507014, -0.53106212, -0.52705411, -0.52304609],\n",
              "       [-0.51903808, -0.51503006, -0.51102204, -0.50701403, -0.50300601,\n",
              "        -0.498998  , -0.49498998, -0.49098196, -0.48697395, -0.48296593],\n",
              "       [-0.47895792, -0.4749499 , -0.47094188, -0.46693387, -0.46292585,\n",
              "        -0.45891784, -0.45490982, -0.4509018 , -0.44689379, -0.44288577],\n",
              "       [-0.43887776, -0.43486974, -0.43086172, -0.42685371, -0.42284569,\n",
              "        -0.41883768, -0.41482966, -0.41082164, -0.40681363, -0.40280561],\n",
              "       [-0.3987976 , -0.39478958, -0.39078156, -0.38677355, -0.38276553,\n",
              "        -0.37875752, -0.3747495 , -0.37074148, -0.36673347, -0.36272545],\n",
              "       [-0.35871743, -0.35470942, -0.3507014 , -0.34669339, -0.34268537,\n",
              "        -0.33867735, -0.33466934, -0.33066132, -0.32665331, -0.32264529],\n",
              "       [-0.31863727, -0.31462926, -0.31062124, -0.30661323, -0.30260521,\n",
              "        -0.29859719, -0.29458918, -0.29058116, -0.28657315, -0.28256513],\n",
              "       [-0.27855711, -0.2745491 , -0.27054108, -0.26653307, -0.26252505,\n",
              "        -0.25851703, -0.25450902, -0.250501  , -0.24649299, -0.24248497],\n",
              "       [-0.23847695, -0.23446894, -0.23046092, -0.22645291, -0.22244489,\n",
              "        -0.21843687, -0.21442886, -0.21042084, -0.20641283, -0.20240481],\n",
              "       [-0.19839679, -0.19438878, -0.19038076, -0.18637275, -0.18236473,\n",
              "        -0.17835671, -0.1743487 , -0.17034068, -0.16633267, -0.16232465],\n",
              "       [-0.15831663, -0.15430862, -0.1503006 , -0.14629259, -0.14228457,\n",
              "        -0.13827655, -0.13426854, -0.13026052, -0.12625251, -0.12224449],\n",
              "       [-0.11823647, -0.11422846, -0.11022044, -0.10621242, -0.10220441,\n",
              "        -0.09819639, -0.09418838, -0.09018036, -0.08617234, -0.08216433],\n",
              "       [-0.07815631, -0.0741483 , -0.07014028, -0.06613226, -0.06212425,\n",
              "        -0.05811623, -0.05410822, -0.0501002 , -0.04609218, -0.04208417],\n",
              "       [-0.03807615, -0.03406814, -0.03006012, -0.0260521 , -0.02204409,\n",
              "        -0.01803607, -0.01402806, -0.01002004, -0.00601202, -0.00200401],\n",
              "       [ 0.00200401,  0.00601202,  0.01002004,  0.01402806,  0.01803607,\n",
              "         0.02204409,  0.0260521 ,  0.03006012,  0.03406814,  0.03807615],\n",
              "       [ 0.04208417,  0.04609218,  0.0501002 ,  0.05410822,  0.05811623,\n",
              "         0.06212425,  0.06613226,  0.07014028,  0.0741483 ,  0.07815631],\n",
              "       [ 0.08216433,  0.08617234,  0.09018036,  0.09418838,  0.09819639,\n",
              "         0.10220441,  0.10621242,  0.11022044,  0.11422846,  0.11823647],\n",
              "       [ 0.12224449,  0.12625251,  0.13026052,  0.13426854,  0.13827655,\n",
              "         0.14228457,  0.14629259,  0.1503006 ,  0.15430862,  0.15831663],\n",
              "       [ 0.16232465,  0.16633267,  0.17034068,  0.1743487 ,  0.17835671,\n",
              "         0.18236473,  0.18637275,  0.19038076,  0.19438878,  0.19839679],\n",
              "       [ 0.20240481,  0.20641283,  0.21042084,  0.21442886,  0.21843687,\n",
              "         0.22244489,  0.22645291,  0.23046092,  0.23446894,  0.23847695],\n",
              "       [ 0.24248497,  0.24649299,  0.250501  ,  0.25450902,  0.25851703,\n",
              "         0.26252505,  0.26653307,  0.27054108,  0.2745491 ,  0.27855711],\n",
              "       [ 0.28256513,  0.28657315,  0.29058116,  0.29458918,  0.29859719,\n",
              "         0.30260521,  0.30661323,  0.31062124,  0.31462926,  0.31863727],\n",
              "       [ 0.32264529,  0.32665331,  0.33066132,  0.33466934,  0.33867735,\n",
              "         0.34268537,  0.34669339,  0.3507014 ,  0.35470942,  0.35871743],\n",
              "       [ 0.36272545,  0.36673347,  0.37074148,  0.3747495 ,  0.37875752,\n",
              "         0.38276553,  0.38677355,  0.39078156,  0.39478958,  0.3987976 ],\n",
              "       [ 0.40280561,  0.40681363,  0.41082164,  0.41482966,  0.41883768,\n",
              "         0.42284569,  0.42685371,  0.43086172,  0.43486974,  0.43887776],\n",
              "       [ 0.44288577,  0.44689379,  0.4509018 ,  0.45490982,  0.45891784,\n",
              "         0.46292585,  0.46693387,  0.47094188,  0.4749499 ,  0.47895792],\n",
              "       [ 0.48296593,  0.48697395,  0.49098196,  0.49498998,  0.498998  ,\n",
              "         0.50300601,  0.50701403,  0.51102204,  0.51503006,  0.51903808],\n",
              "       [ 0.52304609,  0.52705411,  0.53106212,  0.53507014,  0.53907816,\n",
              "         0.54308617,  0.54709419,  0.5511022 ,  0.55511022,  0.55911824],\n",
              "       [ 0.56312625,  0.56713427,  0.57114228,  0.5751503 ,  0.57915832,\n",
              "         0.58316633,  0.58717435,  0.59118236,  0.59519038,  0.5991984 ],\n",
              "       [ 0.60320641,  0.60721443,  0.61122244,  0.61523046,  0.61923848,\n",
              "         0.62324649,  0.62725451,  0.63126253,  0.63527054,  0.63927856],\n",
              "       [ 0.64328657,  0.64729459,  0.65130261,  0.65531062,  0.65931864,\n",
              "         0.66332665,  0.66733467,  0.67134269,  0.6753507 ,  0.67935872],\n",
              "       [ 0.68336673,  0.68737475,  0.69138277,  0.69539078,  0.6993988 ,\n",
              "         0.70340681,  0.70741483,  0.71142285,  0.71543086,  0.71943888],\n",
              "       [ 0.72344689,  0.72745491,  0.73146293,  0.73547094,  0.73947896,\n",
              "         0.74348697,  0.74749499,  0.75150301,  0.75551102,  0.75951904],\n",
              "       [ 0.76352705,  0.76753507,  0.77154309,  0.7755511 ,  0.77955912,\n",
              "         0.78356713,  0.78757515,  0.79158317,  0.79559118,  0.7995992 ],\n",
              "       [ 0.80360721,  0.80761523,  0.81162325,  0.81563126,  0.81963928,\n",
              "         0.82364729,  0.82765531,  0.83166333,  0.83567134,  0.83967936],\n",
              "       [ 0.84368737,  0.84769539,  0.85170341,  0.85571142,  0.85971944,\n",
              "         0.86372745,  0.86773547,  0.87174349,  0.8757515 ,  0.87975952],\n",
              "       [ 0.88376754,  0.88777555,  0.89178357,  0.89579158,  0.8997996 ,\n",
              "         0.90380762,  0.90781563,  0.91182365,  0.91583166,  0.91983968],\n",
              "       [ 0.9238477 ,  0.92785571,  0.93186373,  0.93587174,  0.93987976,\n",
              "         0.94388778,  0.94789579,  0.95190381,  0.95591182,  0.95991984],\n",
              "       [ 0.96392786,  0.96793587,  0.97194389,  0.9759519 ,  0.97995992,\n",
              "         0.98396794,  0.98797595,  0.99198397,  0.99599198,  1.        ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "metadata": {
        "id": "wpX7hHmzrIzZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "49179ee7-5ee5-40fa-f8e6-02a14cec54c1"
      },
      "cell_type": "code",
      "source": [
        "answers = np.arange(50)%10\n",
        "answers"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1,\n",
              "       2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3,\n",
              "       4, 5, 6, 7, 8, 9])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "metadata": {
        "id": "job6XF1_rNUy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "5a2da70a-075d-4a3f-94a8-903176251512"
      },
      "cell_type": "code",
      "source": [
        "softmax_crossentropy_with_logits(logits,answers)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([2.32068743, 2.31667941, 2.3126714 , 2.30866338, 2.30465536,\n",
              "       2.30064735, 2.29663933, 2.29263132, 2.2886233 , 2.28461528,\n",
              "       2.32068743, 2.31667941, 2.3126714 , 2.30866338, 2.30465536,\n",
              "       2.30064735, 2.29663933, 2.29263132, 2.2886233 , 2.28461528,\n",
              "       2.32068743, 2.31667941, 2.3126714 , 2.30866338, 2.30465536,\n",
              "       2.30064735, 2.29663933, 2.29263132, 2.2886233 , 2.28461528,\n",
              "       2.32068743, 2.31667941, 2.3126714 , 2.30866338, 2.30465536,\n",
              "       2.30064735, 2.29663933, 2.29263132, 2.2886233 , 2.28461528,\n",
              "       2.32068743, 2.31667941, 2.3126714 , 2.30866338, 2.30465536,\n",
              "       2.30064735, 2.29663933, 2.29263132, 2.2886233 , 2.28461528])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "metadata": {
        "id": "FOnwKQt_ranR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1717
        },
        "outputId": "9c364825-12bb-4328-e3df-9ac43305146a"
      },
      "cell_type": "code",
      "source": [
        "grads = grad_softmax_crossentropy_with_logits(logits,answers)\n",
        "grads"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.01803588,  0.00197201,  0.00197993,  0.00198788,  0.00199586,\n",
              "         0.00200388,  0.00201193,  0.00202001,  0.00202812,  0.00203626],\n",
              "       [ 0.00196412, -0.01802799,  0.00197993,  0.00198788,  0.00199586,\n",
              "         0.00200388,  0.00201193,  0.00202001,  0.00202812,  0.00203626],\n",
              "       [ 0.00196412,  0.00197201, -0.01802007,  0.00198788,  0.00199586,\n",
              "         0.00200388,  0.00201193,  0.00202001,  0.00202812,  0.00203626],\n",
              "       [ 0.00196412,  0.00197201,  0.00197993, -0.01801212,  0.00199586,\n",
              "         0.00200388,  0.00201193,  0.00202001,  0.00202812,  0.00203626],\n",
              "       [ 0.00196412,  0.00197201,  0.00197993,  0.00198788, -0.01800414,\n",
              "         0.00200388,  0.00201193,  0.00202001,  0.00202812,  0.00203626],\n",
              "       [ 0.00196412,  0.00197201,  0.00197993,  0.00198788,  0.00199586,\n",
              "        -0.01799612,  0.00201193,  0.00202001,  0.00202812,  0.00203626],\n",
              "       [ 0.00196412,  0.00197201,  0.00197993,  0.00198788,  0.00199586,\n",
              "         0.00200388, -0.01798807,  0.00202001,  0.00202812,  0.00203626],\n",
              "       [ 0.00196412,  0.00197201,  0.00197993,  0.00198788,  0.00199586,\n",
              "         0.00200388,  0.00201193, -0.01797999,  0.00202812,  0.00203626],\n",
              "       [ 0.00196412,  0.00197201,  0.00197993,  0.00198788,  0.00199586,\n",
              "         0.00200388,  0.00201193,  0.00202001, -0.01797188,  0.00203626],\n",
              "       [ 0.00196412,  0.00197201,  0.00197993,  0.00198788,  0.00199586,\n",
              "         0.00200388,  0.00201193,  0.00202001,  0.00202812, -0.01796374],\n",
              "       [-0.01803588,  0.00197201,  0.00197993,  0.00198788,  0.00199586,\n",
              "         0.00200388,  0.00201193,  0.00202001,  0.00202812,  0.00203626],\n",
              "       [ 0.00196412, -0.01802799,  0.00197993,  0.00198788,  0.00199586,\n",
              "         0.00200388,  0.00201193,  0.00202001,  0.00202812,  0.00203626],\n",
              "       [ 0.00196412,  0.00197201, -0.01802007,  0.00198788,  0.00199586,\n",
              "         0.00200388,  0.00201193,  0.00202001,  0.00202812,  0.00203626],\n",
              "       [ 0.00196412,  0.00197201,  0.00197993, -0.01801212,  0.00199586,\n",
              "         0.00200388,  0.00201193,  0.00202001,  0.00202812,  0.00203626],\n",
              "       [ 0.00196412,  0.00197201,  0.00197993,  0.00198788, -0.01800414,\n",
              "         0.00200388,  0.00201193,  0.00202001,  0.00202812,  0.00203626],\n",
              "       [ 0.00196412,  0.00197201,  0.00197993,  0.00198788,  0.00199586,\n",
              "        -0.01799612,  0.00201193,  0.00202001,  0.00202812,  0.00203626],\n",
              "       [ 0.00196412,  0.00197201,  0.00197993,  0.00198788,  0.00199586,\n",
              "         0.00200388, -0.01798807,  0.00202001,  0.00202812,  0.00203626],\n",
              "       [ 0.00196412,  0.00197201,  0.00197993,  0.00198788,  0.00199586,\n",
              "         0.00200388,  0.00201193, -0.01797999,  0.00202812,  0.00203626],\n",
              "       [ 0.00196412,  0.00197201,  0.00197993,  0.00198788,  0.00199586,\n",
              "         0.00200388,  0.00201193,  0.00202001, -0.01797188,  0.00203626],\n",
              "       [ 0.00196412,  0.00197201,  0.00197993,  0.00198788,  0.00199586,\n",
              "         0.00200388,  0.00201193,  0.00202001,  0.00202812, -0.01796374],\n",
              "       [-0.01803588,  0.00197201,  0.00197993,  0.00198788,  0.00199586,\n",
              "         0.00200388,  0.00201193,  0.00202001,  0.00202812,  0.00203626],\n",
              "       [ 0.00196412, -0.01802799,  0.00197993,  0.00198788,  0.00199586,\n",
              "         0.00200388,  0.00201193,  0.00202001,  0.00202812,  0.00203626],\n",
              "       [ 0.00196412,  0.00197201, -0.01802007,  0.00198788,  0.00199586,\n",
              "         0.00200388,  0.00201193,  0.00202001,  0.00202812,  0.00203626],\n",
              "       [ 0.00196412,  0.00197201,  0.00197993, -0.01801212,  0.00199586,\n",
              "         0.00200388,  0.00201193,  0.00202001,  0.00202812,  0.00203626],\n",
              "       [ 0.00196412,  0.00197201,  0.00197993,  0.00198788, -0.01800414,\n",
              "         0.00200388,  0.00201193,  0.00202001,  0.00202812,  0.00203626],\n",
              "       [ 0.00196412,  0.00197201,  0.00197993,  0.00198788,  0.00199586,\n",
              "        -0.01799612,  0.00201193,  0.00202001,  0.00202812,  0.00203626],\n",
              "       [ 0.00196412,  0.00197201,  0.00197993,  0.00198788,  0.00199586,\n",
              "         0.00200388, -0.01798807,  0.00202001,  0.00202812,  0.00203626],\n",
              "       [ 0.00196412,  0.00197201,  0.00197993,  0.00198788,  0.00199586,\n",
              "         0.00200388,  0.00201193, -0.01797999,  0.00202812,  0.00203626],\n",
              "       [ 0.00196412,  0.00197201,  0.00197993,  0.00198788,  0.00199586,\n",
              "         0.00200388,  0.00201193,  0.00202001, -0.01797188,  0.00203626],\n",
              "       [ 0.00196412,  0.00197201,  0.00197993,  0.00198788,  0.00199586,\n",
              "         0.00200388,  0.00201193,  0.00202001,  0.00202812, -0.01796374],\n",
              "       [-0.01803588,  0.00197201,  0.00197993,  0.00198788,  0.00199586,\n",
              "         0.00200388,  0.00201193,  0.00202001,  0.00202812,  0.00203626],\n",
              "       [ 0.00196412, -0.01802799,  0.00197993,  0.00198788,  0.00199586,\n",
              "         0.00200388,  0.00201193,  0.00202001,  0.00202812,  0.00203626],\n",
              "       [ 0.00196412,  0.00197201, -0.01802007,  0.00198788,  0.00199586,\n",
              "         0.00200388,  0.00201193,  0.00202001,  0.00202812,  0.00203626],\n",
              "       [ 0.00196412,  0.00197201,  0.00197993, -0.01801212,  0.00199586,\n",
              "         0.00200388,  0.00201193,  0.00202001,  0.00202812,  0.00203626],\n",
              "       [ 0.00196412,  0.00197201,  0.00197993,  0.00198788, -0.01800414,\n",
              "         0.00200388,  0.00201193,  0.00202001,  0.00202812,  0.00203626],\n",
              "       [ 0.00196412,  0.00197201,  0.00197993,  0.00198788,  0.00199586,\n",
              "        -0.01799612,  0.00201193,  0.00202001,  0.00202812,  0.00203626],\n",
              "       [ 0.00196412,  0.00197201,  0.00197993,  0.00198788,  0.00199586,\n",
              "         0.00200388, -0.01798807,  0.00202001,  0.00202812,  0.00203626],\n",
              "       [ 0.00196412,  0.00197201,  0.00197993,  0.00198788,  0.00199586,\n",
              "         0.00200388,  0.00201193, -0.01797999,  0.00202812,  0.00203626],\n",
              "       [ 0.00196412,  0.00197201,  0.00197993,  0.00198788,  0.00199586,\n",
              "         0.00200388,  0.00201193,  0.00202001, -0.01797188,  0.00203626],\n",
              "       [ 0.00196412,  0.00197201,  0.00197993,  0.00198788,  0.00199586,\n",
              "         0.00200388,  0.00201193,  0.00202001,  0.00202812, -0.01796374],\n",
              "       [-0.01803588,  0.00197201,  0.00197993,  0.00198788,  0.00199586,\n",
              "         0.00200388,  0.00201193,  0.00202001,  0.00202812,  0.00203626],\n",
              "       [ 0.00196412, -0.01802799,  0.00197993,  0.00198788,  0.00199586,\n",
              "         0.00200388,  0.00201193,  0.00202001,  0.00202812,  0.00203626],\n",
              "       [ 0.00196412,  0.00197201, -0.01802007,  0.00198788,  0.00199586,\n",
              "         0.00200388,  0.00201193,  0.00202001,  0.00202812,  0.00203626],\n",
              "       [ 0.00196412,  0.00197201,  0.00197993, -0.01801212,  0.00199586,\n",
              "         0.00200388,  0.00201193,  0.00202001,  0.00202812,  0.00203626],\n",
              "       [ 0.00196412,  0.00197201,  0.00197993,  0.00198788, -0.01800414,\n",
              "         0.00200388,  0.00201193,  0.00202001,  0.00202812,  0.00203626],\n",
              "       [ 0.00196412,  0.00197201,  0.00197993,  0.00198788,  0.00199586,\n",
              "        -0.01799612,  0.00201193,  0.00202001,  0.00202812,  0.00203626],\n",
              "       [ 0.00196412,  0.00197201,  0.00197993,  0.00198788,  0.00199586,\n",
              "         0.00200388, -0.01798807,  0.00202001,  0.00202812,  0.00203626],\n",
              "       [ 0.00196412,  0.00197201,  0.00197993,  0.00198788,  0.00199586,\n",
              "         0.00200388,  0.00201193, -0.01797999,  0.00202812,  0.00203626],\n",
              "       [ 0.00196412,  0.00197201,  0.00197993,  0.00198788,  0.00199586,\n",
              "         0.00200388,  0.00201193,  0.00202001, -0.01797188,  0.00203626],\n",
              "       [ 0.00196412,  0.00197201,  0.00197993,  0.00198788,  0.00199586,\n",
              "         0.00200388,  0.00201193,  0.00202001,  0.00202812, -0.01796374]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "metadata": {
        "id": "aK62PGU9k7nY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "logits = np.linspace(-1,1,500).reshape([50,10])\n",
        "answers = np.arange(50)%10\n",
        "\n",
        "softmax_crossentropy_with_logits(logits,answers)\n",
        "grads = grad_softmax_crossentropy_with_logits(logits,answers)\n",
        "numeric_grads = eval_numerical_gradient(lambda l: softmax_crossentropy_with_logits(l,answers).mean(),logits)\n",
        "\n",
        "assert np.allclose(numeric_grads,grads,rtol=1e-3,atol=0), \"The reference implementation has just failed. Someone has just changed the rules of math.\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YtDxg--6k7ni",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Full network\n",
        "\n",
        "Now let's combine what we've just built into a working neural network. As we announced, we're gonna use this monster to classify handwritten digits, so let's get them loaded."
      ]
    },
    {
      "metadata": {
        "id": "20mFVcLdk7nl",
        "colab_type": "code",
        "outputId": "952619c7-4d12-4984-a434-766ea7c3320d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 389
        }
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "from mnist import load_dataset\n",
        "X_train, y_train, X_val, y_val, X_test, y_test = load_dataset(flatten=True)\n",
        "\n",
        "plt.figure(figsize=[6,6])\n",
        "for i in range(4):\n",
        "    plt.subplot(2,2,i+1)\n",
        "    plt.title(\"Label: %i\"%y_train[i])\n",
        "    plt.imshow(X_train[i].reshape([28,28]),cmap='gray');"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWsAAAF0CAYAAADlxhg2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7facbfefe080>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "nHO1MLWTk7ol",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We'll define network as a list of layers, each applied on top of previous one. In this setting, computing predictions and training becomes trivial."
      ]
    },
    {
      "metadata": {
        "id": "ugDLU5tck7op",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "network = []\n",
        "network.append(Dense(X_train.shape[1], 100))\n",
        "network.append(ReLU())\n",
        "network.append(Dense(100, 200))\n",
        "network.append(ReLU())\n",
        "network.append(Dense(200, 10))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IrWF4GCjk7o-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def forward(network, X):\n",
        "    \"\"\"\n",
        "    Compute activations of all network layers by applying them sequentially.\n",
        "    Return a list of activations for each layer. \n",
        "    Make sure last activation corresponds to network logits.\n",
        "    \"\"\"\n",
        "    activations = []\n",
        "    input = X\n",
        "\n",
        "    for l in network:\n",
        "      activations.append(l.forward(input))\n",
        "      input = activations[-1]\n",
        "        \n",
        "    assert len(activations) == len(network)\n",
        "    return activations\n",
        "\n",
        "def predict(network, X):\n",
        "    \"\"\"\n",
        "    Use network to predict the most likely class for each sample.\n",
        "    \"\"\"\n",
        "    logits = forward(network, X)[-1]\n",
        "    return logits.argmax(axis=-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6CELTHsBk7pk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Backprop\n",
        "\n",
        "You can now define the backpropagation step for the neural network. Please read the docstring."
      ]
    },
    {
      "metadata": {
        "id": "cOtuNL9xk7pn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train(network,X,y):\n",
        "    \"\"\"\n",
        "    Train your network on a given batch of X and y.\n",
        "    You first need to run forward to get all layer activations.\n",
        "    You can estimate loss and loss_grad, obtaining dL / dy_pred\n",
        "    Then you can run layer.backward going from last layer to first, \n",
        "    propagating the gradient of input to previous layers.\n",
        "    \n",
        "    After you called backward for all layers, all Dense layers have already made one gradient step.\n",
        "    \"\"\"\n",
        "    \n",
        "    # Get the layer activations\n",
        "    layer_activations = forward(network,X)\n",
        "    layer_inputs = [X] + layer_activations  #layer_input[i] is an input for network[i]\n",
        "    logits = layer_activations[-1]\n",
        "    \n",
        "    # Compute the loss and the initial gradient\n",
        "    loss = softmax_crossentropy_with_logits(logits,y)\n",
        "    loss_grad = grad_softmax_crossentropy_with_logits(logits,y)\n",
        "    \n",
        "    \n",
        "    # propagate gradients through network layers using .backward\n",
        "    # hint: start from last layer and move to earlier layers\n",
        "    for layer_index in range(len(network))[::-1]:\n",
        "        layer = network[layer_index]\n",
        "        \n",
        "        loss_grad = layer.backward(layer_inputs[layer_index],loss_grad) #grad w.r.t. input, also weight updates\n",
        "        \n",
        "    return np.mean(loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Suab2P0Tk7p9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Instead of tests, we provide you with a training loop that prints training and validation accuracies on every epoch.\n",
        "\n",
        "If your implementation of forward and backward are correct, your accuracy should grow from 90~93% to >97% with the default network."
      ]
    },
    {
      "metadata": {
        "id": "cnKyARFJk7qA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Training loop\n",
        "\n",
        "As usual, we split data into minibatches, feed each such minibatch into the network and update weights."
      ]
    },
    {
      "metadata": {
        "id": "uQrq8QMyk7qO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from tqdm import trange\n",
        "def iterate_minibatches(inputs, targets, batchsize, shuffle=False):\n",
        "    assert len(inputs) == len(targets)\n",
        "    if shuffle:\n",
        "        indices = np.random.permutation(len(inputs))\n",
        "    for start_idx in trange(0, len(inputs) - batchsize + 1, batchsize):\n",
        "        if shuffle:\n",
        "            excerpt = indices[start_idx:start_idx + batchsize]\n",
        "        else:\n",
        "            excerpt = slice(start_idx, start_idx + batchsize)\n",
        "        yield inputs[excerpt], targets[excerpt]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "u9aIIfkFk7qp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from IPython.display import clear_output\n",
        "train_log = []\n",
        "val_log = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "l_GuK-uDk7q4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "outputId": "9a7d97af-f178-4b2a-ee47-950a46aeffd5"
      },
      "cell_type": "code",
      "source": [
        "for epoch in range(25):\n",
        "\n",
        "    for x_batch,y_batch in iterate_minibatches(X_train, y_train, batchsize=32, shuffle=True):\n",
        "        train(network, x_batch, y_batch)\n",
        "    \n",
        "    train_log.append(np.mean(predict(network, X_train) == y_train))\n",
        "    val_log.append(np.mean(predict(network, X_val) == y_val))\n",
        "    \n",
        "    clear_output()\n",
        "    print(\"Epoch\",epoch)\n",
        "    print(\"Train accuracy:\",train_log[-1])\n",
        "    print(\"Val accuracy:\",val_log[-1])\n",
        "    plt.plot(train_log,label='train accuracy')\n",
        "    plt.plot(val_log,label='val accuracy')\n",
        "    plt.legend(loc='best')\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "    "
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 24\n",
            "Train accuracy: 1.0\n",
            "Val accuracy: 0.9799\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD4CAYAAADiry33AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7facbc6865f8>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "fKS5KpPKLHNk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "B4zZXvN2k7q_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "__What should you see:__ `train accuracy` should increase to near-100%. Val accuracy will also increase, allbeit to a smaller value.\n",
        "\n",
        "__What else to try:__ You can try implementing different nonlinearities, dropout or composing neural network of more layers. See how this affects training speed, overfitting & final quality.\n",
        "\n",
        "Good hunting!"
      ]
    },
    {
      "metadata": {
        "id": "cl55zvQHk7rF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# and yes, it's perfectly legal to reuse your code from this seminar in homework01."
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}